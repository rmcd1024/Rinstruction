---
title: "R four ways (plus a few)"
author: |
    | Robert McDonald
    | Kellogg School, Northwestern University
date: "`r Sys.Date()`"
output:
  html_document: default
  beamer_presentation:
    slide_level: 2
    includes:
      in_header: header-pageno.tex
toc: true
urlcolor: 'cyan'
## following is workaround for pdfpages/xcolor package clash
documentclass: beamer
classoption: xcolor=dvipsnames
---

```{r setup, include=FALSE}
evalsql <- FALSE; conn <- FALSE
evalsql <- TRUE
library(knitr)
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      comment = NA, 
                      message = FALSE,
                      warning = FALSE
                      ,cache = TRUE
                      )
evalbool=TRUE
## evalbool=FALSE
load('~/git/rmcd/Rinstruction/password.Rdata')
```

```{r, echo=FALSE}
library(tidyverse)
library(dbplyr)
library(DBI)
library(RPostgreSQL)
```

## The babynames data

* The Social Security Administration provides [state-level babynames
data](https://www.ssa.gov/oact/babynames/state/namesbystate.zip)
annually since 1910.

* One shell command creates  a single file containing all 
state-level babyname data
```
cat *.TXT > allstates.TXT
```

* The resulting file has 5.8 million rows and no header.

## File contents

* Using the `head` command (in Bash):

\footnotesize

```{bash}
head -n 6 data/allstates.TXT
```
\normalsize

The fields (i.e., columns) are:

* state, a two-digit abbreviation
* sex, M or F
* year, yyyy
* name
* number of births

## The tasks

* Four basic data manipulation tasks:

1. Count the number of distinct states in the data
2. Count the number of distinct years in the data
2. Count the number of distinct names in the data
1. Create a new CSV file that contains the top 10 names nationally, by
sex, for each year.

* We will use Base R with and without loops, `dplyr`, and `data.table`
* We will find that `dplyr` is faster than base R, and `data.table` is
  faster still

# Base R

## R: Base R

* Many argue that looping in  R is always slow
* Beginning users often want to write loops
* In this problem, there are loops with few iterations (over states and
  sex) and many iterations (over names)
* There is a big gain to replacing the `name` loop, not so much of a
  gain from replacing the other loops


## Using only loops

* The following code is *very* slow. Experienced R users will wince...


\tiny

```{r solnbase_loop2, eval=FALSE}
x = read.csv('data/allstates.TXT', header=FALSE,
             stringsAsFactors=FALSE)
names(x) <- c('state', 'sex', 'year', 'name', 'n')
print(length(table(x$state))) 
print(length(table(x$year)))
print(length(table(x$name)))
top10 <- list()
sexes <- unique(x$sex)
for (i in unique(x$year)) {
    for (j in sexes) {
        tmp <- x[x$year == i & x$sex == j, ]
        names <- unique(tmp$name)
        lnames <- length(names)
        nvec <- vector(length=lnames)
        for (k in 1:lnames) {
            nvec[k] <- sum(tmp$n[tmp$name == names[k]])
        }
        tmp <- data.frame(year=i, sex=j, name=names, n=nvec)
        tmp <- tmp[order(-tmp$n), ]
        top10 <- rbind(top10, head(tmp, n=10), make.row.names=FALSE)
    }
}
write.csv(top10, file='data/babynames10Rbase_loop2.csv',
          row.names=FALSE)
print(head(top10))
```

\normalsize

## Results: R with explicit loops


\footnotesize

```{r time_solnbase_loop2, echo=FALSE}
system.time({
<<solnbase_loop2>>
})
```
\normalsize


## R with some loops

* We can use the `aggregate` function to replace the innermost loop,
  which sums individual names across states for a given year and sex
* Using `aggregate` we apply the function `sum` to the formula `n ~
  year + sex + name`

\footnotesize

```{r, eval=FALSE}
xa <- aggregate(n ~ year + sex + name, data=x, FUN=sum)
```

\normalsize

* This is substantially faster

## R with some loops


\footnotesize

```{r solnbase_loop, eval=FALSE}
x = read.csv('data/allstates.TXT', header=FALSE,
             stringsAsFactors=FALSE)
names(x) <- c('state', 'sex', 'year', 'name', 'n')
print(length(table(x$state))) 
print(length(table(x$year)))
print(length(table(x$name)))
xa <- aggregate(n ~ year + sex + name, data=x, FUN=sum)
xa <- xa[order(xa$year, xa$sex, -xa$n), ]
top10 <- data.frame()
for (i in unique(xa$year)) {
    for (j in unique(xa$sex)) {
        tmp = head(subset(xa, xa$year == i & xa$sex == j), n=10)
        top10 <- rbind(top10, tmp, make.row.names=FALSE)
  }
}
write.csv(top10, file='data/babynames10Rbase_loop.csv',
          row.names=FALSE)
print(head(top10))

```

\normalsize



## Results: R with some loops


\footnotesize

```{r time_solnbase_loop, echo=FALSE}
system.time({
<<solnbase_loop>>
})
```
\normalsize


## Without explicit loops

* Using `by` to replace the year/sex loops leads to cleaner, more
  "R-ish" code.
* Perhaps less transparent? It is only slightly faster

\footnotesize


```{r solnbase_noloop, eval=FALSE, echo=TRUE}
x = read.csv('data/allstates.TXT', header=FALSE,
             stringsAsFactors=FALSE)
names(x) <- c('state', 'sex', 'year', 'name', 'n')
print(length(table(x$state))) ## note the similarity to python
print(length(table(x$year)))
print(length(table(x$name)))
xa <- aggregate(n ~ year + sex + name, data=x, FUN=sum)
xa <- xa[order(xa$year, xa$sex, -xa$n), ]
xatop10 <- by(xa, list(xa$year, xa$sex), head, n=10)
top10 <- do.call(rbind, xatop10)
write.csv(top10, file='data/babynames10Rbase_noloop.csv',
          row.names=FALSE)
print(head(top10))
```

\normalsize

## Results: R without explicit loops

\footnotesize

```{r time_solnbase_noloop, echo=FALSE}
system.time({
<<solnbase_noloop>>
})
```

# `dplyr`

\normalsize

## The `dplyr` approach

* The `dplyr` package permits manipulation data with echoes of
  SQL. 
* There are explicit "verbs" for data manipulation tasks (sorting,
  filtering by row, selecting columns, grouping, summarizing, etc.)
* `dplyr` is very fast to code
* Compare the `dplyr` code to the "no-loop" base R code

## `dplyr` and `tidy` {#dplyr}


\footnotesize

```{r soln_dplyr, eval=FALSE}
x <- read_csv('data/allstates.TXT',
             col_names=c('state', 'sex', 'year', 'name', 'n'),
             col_types = cols(sex = col_character())
             )
print(nrow(distinct(x, state)))
print(nrow(distinct(x, year)))
print(nrow(distinct(x, name)))
out = x %>% 
  group_by(year, sex, name) %>% 
  summarize(n = sum(n)) %>% 
  arrange(year, sex, desc(n)) %>%
  filter(row_number(desc(n)) <= 10) 
##  do(head(., n=10))  ## works in place of filter
write_csv(out, path='data/babynames10Rdplyr.csv')
print(head(out))
```

\normalsize

## Results: dplyr

\footnotesize

```{r soln_dplyr_time, eval=TRUE, echo=FALSE}
system.time({
<<soln_dplyr>>
})
```

\normalsize

\newpage

## R: `dplyr` with `map` {#dplyr_map}

* The `purrr` functions `nest` and `map` can also be used.

\footnotesize

```{r soln_dplyr_map, eval=FALSE}
x <- read_csv('data/allstates.TXT',
             col_names=c('state', 'sex', 'year', 'name', 'n'),
             col_types = cols(sex = col_character())
             )
print(nrow(distinct(x, state)))
print(nrow(distinct(x, year)))
print(nrow(distinct(x, name)))
out = x %>% 
  group_by(year, sex, name) %>% 
  summarize(n = sum(n)) %>% 
  arrange(year, sex, desc(n)) %>%
  nest() %>%  ## will nest on the grouping variables
  map_df(.x=.$data, .f=head, n=10)
write_csv(out, path='data/babynames10Rdplyrmap.csv')
print(head(out))
```

\normalsize

## Results: `dplyr` with `map`

\footnotesize

```{r soln_dplyr_map_time, eval=TRUE, echo=FALSE}
system.time({
<<soln_dplyr_map>>
})
```

\normalsize

# The `data.table` package

\newpage

## `data.table`

* `data.table` is designed explicitly for manipulation of large data
sets.  The syntax is more abstract than in `dplyr`
* Like dplyr, it permits chaining commands. 
* For a data table, `DT`, with row `i`, column `j`, grouped by `by`,
  the syntax is `DT[i, j, by]`

\footnotesize

```{r soln_datatable, eval=FALSE}
library(data.table)
y <- fread("data/allstates.TXT",
           col.names=c('state', 'sex', 'year', 'name', 'n'))
print(y[, uniqueN(state)])
print(y[, uniqueN(year)])
print(y[, uniqueN(name)])
y2 = y[, .(total = sum(n)), by=.(year, sex, name)][
  order(year, sex, -total)]
out = y2[, head(.SD, 10),  by=.(year, sex)]
fwrite(out, file='data/babynames10RDT.csv')
print(head(out))
```

\normalsize

## Results: `data.table`

\footnotesize

```{r soln_datatable_time, echo=FALSE}
system.time({
<<soln_datatable>>
})    
```

\normalsize

# Reading and writing data files


\newpage

## `read.csv` vs `read_csv` vs `fread` {#r-csv-speed}

* In each comparison we have used the "native" function for reading
  and writing in that particular environment
* `read.csv` and `write.csv` are in  base R
* `read_csv` and `write_csv` are in `dplyr`
* `fread` and `fwrite` are in `data.table`

## Comparison: Reading

* Note that `read_csv` without column types will throw an error
because it infers that the variable `sex` is logical

\tiny

```{r}
system.time(x <- read.csv('data/allstates.TXT',
                          header=FALSE))
system.time(
    x <- read_csv('data/allstates.TXT',
                  progress=FALSE,
                  col_names=c('state', 'sex', 'year', 'name', 'n'),
                  col_types = cols(sex = col_character())
                  )
)
system.time(x <- fread("data/allstates.TXT",
                       col.names=c('state', 'sex', 'year', 'name', 'n'))
            )

```

\normalsize

## Writing files

* We can choose

	1. Writing CSV or Rdata files
	2. If Rdata: Writing compressed or uncompressed
	3. If CSV: Using one of three functions
  

## Comparison: Writing


\tiny
   
```{r time_write}
system.time(write.csv(x, file='/tmp/save1.CSV'))
system.time(write_csv(x, path='/tmp/save2.CSV'))
system.time(fwrite(x, file='/tmp/save3.CSV'))
system.time(save(x, file='/tmp/save.Rdata'))
system.time(save(x, file='/tmp/save2.Rdata', compress=FALSE))
system.time(saveRDS(x, file='/tmp/save.RDS', compress=FALSE))
cat(system('ls -al /tmp/save*', intern=TRUE), sep='\n')
```

\normalsize

## Reading the Rdata files back in

\footnotesize

```{r time_read}
system.time(load('/tmp/save.Rdata'))
system.time(load('/tmp/save2.Rdata'))
system.time(y <- readRDS(file='/tmp/save.RDS'))
```

\normalsize

* The relative times depend on both CPU and disk speeds

## Conclusion about reading and writing

* Use `fread` and `fwrite` if the file is not small
* When writing R files, do not use compression (the default) 
* In the previous examples:
	* differences in file *reading* speed would have been substantial
	* differences in file *writing* speed would have been small,
    because the output file was small
* `dplyr` using `fread` and `fwrite` runs in under 5 seconds

# SQL


\newpage
	
## Creating an SQL Connection

* It is possible to use dplyr with an SQL connection
* SQL databases have their own passwords
	* Password security becomes an issue when creating scripts. Two
solutions are the `keyringr` package, which reads your local keyring,
and the `getPass` package, which will prompt you for the password when
making a connection.)

\footnotesize

```{r sql, message=FALSE, cache=FALSE, eval=evalsql}
conn = DBI::dbConnect(RPostgreSQL::PostgreSQL(),
                      user=username,
                      password=pw, 
                      dbname='babynames_by_state',
                      host='localhost'
                      )
```
\normalsize


\newpage



## R, using a connection to an SQL database

* A database connection can be used with either SQL or R. 
* `dplyr` code works with the remote database

\footnotesize

```{r dplyr_sql, echo=TRUE, eval=FALSE, comment=FALSE}
names.tbl <- tbl(conn, 'names')
distinct(names.tbl, state) %>% count 
distinct(names.tbl, year)  %>% count 
distinct(names.tbl, name) %>% count 
tmp <- names.tbl %>% 
    group_by(year, sex, name) %>%
    summarize(total=sum(n)) %>% 
    arrange(year, sex, -total) %>% 
    filter(row_number() <= 10)
print(head(collect(tmp)))
```

\normalsize

## Results: SQL via `dplyr`

* It's hard to assess the relative speed because the remote SQL engine
and network play a role

\footnotesize

```{r, echo=FALSE, eval=evalsql}
system.time({
<<dplyr_sql>>
})
```

\normalsize


## The `dplyr` query

* Use `show_query()` to examine the query constructed by `dplyr`

\tiny

```{r showquery, message=TRUE, eval=evalsql}
show_query(tmp)
```

\normalsize


## Manipulation using SQL

*  Access the SQL connection by setting `connection=conn` in the the
   chunk options.

\scriptsize

```{sql state, connection=conn, eval=evalsql}
select count(distinct state) from names;
```

```{sql year, connection=conn, eval=evalsql}
select count(distinct year) from names;
```

```{sql names, connection=conn, eval=evalsql}
select count(distinct name) from names;
```

\normalsize


## Direct SQL

* The following chunk is pure SQL. The result of the statement will be
  assigned to the data frame `babynames10sql`, specified in the chunk
  options as `output.var='babynames10sql'`.
  
\scriptsize  
  
```{sql, connection='conn', output.var='babynames10sql', eval=evalsql}
-- name the output with chunk option "output.var='babynames10sql'""
create temp table tmp as
select * from 
(
select  name, year, sex, SUM(n),
ROW_NUMBER () OVER (
PARTITION BY year, sex
order by year, sex, sum(n) desc
) 
from names
group by year, sex, name
order by year, sex, sum desc
) as foo 
where row_number <= 10;
select year, sex, name, sum from tmp;
```

\normalsize


## Back to R to look at the results...

* Now we're using R again.

\footnotesize

```{r, eval=evalsql}
head(babynames10sql)
write_csv(babynames10sql, path='data/babynames10sql.csv')

```

\normalsize

```{r, echo=FALSE, results=FALSE, eval=evalsql}
dbDisconnect(conn)
```

\newpage

# Command line

## Command line

*  You can use the command line to do some of this.
*  The works in Linux, OS X, and Windows with either git-bash or the
[Linux Subsystem for
Windows](https://msdn.microsoft.com/en-us/commandline/wsl/install_guide).

\footnotesize

```{bash, eval=TRUE}
## Number of states
cut -d, -f1 data/allstates.TXT | uniq |  wc -l
```
```{bash}
## Number of years
cut -d, -f3 data/allstates.TXT | uniq | sort | uniq | wc -l
```
```{bash}
## Number of names
cut -d, -f4 data/allstates.TXT | uniq | sort | uniq | wc -l
```

\normalsize

## Conclusions

* Use `dplyr` or `data.table` (especially for large data)
* For large files, save uncompressed
* For CSV files, `data.table::fread` and `data.table::fwrite` are outstanding
* Some loops are okay, but using loops for everything kills
  performance and takes too much time to code
* Learn to use the command line
	* If you are using Linux or OS X, you have what you need
	* If you are using Windows, you will need to install either
      [git-bash](https://git-for-windows.github.io/) or the [Linux
      Subsystem for
      Windows](https://msdn.microsoft.com/en-us/commandline/wsl/install_guide)
      (only for Windows 10)

